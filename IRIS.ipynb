{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.csv news.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "fields = [StructField(field_name, DoubleType(), True) \n",
    "          for field_name in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] + \\\n",
    "    [StructField('species', StringType(), True)]\n",
    "    \n",
    "schema = StructType(fields)\n",
    "\n",
    "iris_df = sqlContext.read.csv(\"dataset/iris.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "str_indexer = StringIndexer(inputCol=\"species\", outputCol=\"species_index\")\n",
    "label_encoder = OneHotEncoder(inputCol=\"species_index\", outputCol=\"labels\")\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "    outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[str_indexer, label_encoder, feature_assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_pipeline = pipeline.fit(iris_df)\n",
    "iris_dataset_df = iris_pipeline.transform(iris_df, {label_encoder.dropLast: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-------------+-------------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|species_index|       labels|         features|\n",
      "+------------+-----------+------------+-----------+-------+-------------+-------------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|          2.0|(3,[2],[1.0])|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|          2.0|(3,[2],[1.0])|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|          2.0|(3,[2],[1.0])|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|          2.0|(3,[2],[1.0])|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|          2.0|(3,[2],[1.0])|[5.0,3.6,1.4,0.2]|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|          2.0|(3,[2],[1.0])|[5.4,3.9,1.7,0.4]|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|          2.0|(3,[2],[1.0])|[4.6,3.4,1.4,0.3]|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|          2.0|(3,[2],[1.0])|[5.0,3.4,1.5,0.2]|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|          2.0|(3,[2],[1.0])|[4.4,2.9,1.4,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|          2.0|(3,[2],[1.0])|[4.9,3.1,1.5,0.1]|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|          2.0|(3,[2],[1.0])|[5.4,3.7,1.5,0.2]|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|          2.0|(3,[2],[1.0])|[4.8,3.4,1.6,0.2]|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|          2.0|(3,[2],[1.0])|[4.8,3.0,1.4,0.1]|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|          2.0|(3,[2],[1.0])|[4.3,3.0,1.1,0.1]|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|          2.0|(3,[2],[1.0])|[5.8,4.0,1.2,0.2]|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|          2.0|(3,[2],[1.0])|[5.7,4.4,1.5,0.4]|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|          2.0|(3,[2],[1.0])|[5.4,3.9,1.3,0.4]|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|          2.0|(3,[2],[1.0])|[5.1,3.5,1.4,0.3]|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|          2.0|(3,[2],[1.0])|[5.7,3.8,1.7,0.3]|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|          2.0|(3,[2],[1.0])|[5.1,3.8,1.5,0.3]|\n",
      "+------------+-----------+------------+-----------+-------+-------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_dataset_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data = iris_dataset_df.select('features', 'labels').rdd \\\n",
    "    .map(lambda r: {f: r[f].toArray().reshape(1, -1) for f in ['features', 'labels']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = iris_data.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[43] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def concate_data(x, y):\n",
    "    dataset = {}\n",
    "    for f in ['features', 'labels']:\n",
    "        dataset[f] = np.concatenate((x[f], y[f]), axis=0)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def next_batch_rdd_maker(data_rdd, batch_size):\n",
    "    data_cnt = float(data_rdd.count())\n",
    "    per_batch_ratios = [batch_size / data_cnt] * int(math.ceil(data_cnt / batch_size))\n",
    "    def next_batch():\n",
    "        current_epoch = 0\n",
    "        while True:\n",
    "#             current_epoch += 1\n",
    "#             if epoch_limit & current_epoch > epoch_limit: \n",
    "#                 break \n",
    "            batches = data_rdd \\\n",
    "                .sortBy(lambda x: np.random.random()) \\\n",
    "                .randomSplit(per_batch_ratios)\n",
    "            for batch in batches:\n",
    "                dataset = batch.reduce(concate_data)\n",
    "                yield dataset['features'], dataset['labels']\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_dict = test_data.reduce(concate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_batch = next_batch_rdd_maker(train_data, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 5. ,  2. ,  3.5,  1. ]]), array([[ 0.,  1.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(next_batch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 16 5\n"
     ]
    }
   ],
   "source": [
    "epoches = 20\n",
    "data_cnt = train_data.count()\n",
    "batch_size = 16\n",
    "n_batch = data_cnt // batch_size\n",
    "\n",
    "print(data_cnt, batch_size, n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, test accuracy: 0.637681186199\n",
      "epoch: 2, test accuracy: 0.637681186199\n",
      "epoch: 3, test accuracy: 0.637681186199\n",
      "epoch: 4, test accuracy: 0.637681186199\n",
      "epoch: 5, test accuracy: 0.942028999329\n",
      "epoch: 6, test accuracy: 0.753623187542\n",
      "epoch: 7, test accuracy: 0.956521749496\n",
      "epoch: 8, test accuracy: 0.942028999329\n",
      "epoch: 9, test accuracy: 0.956521749496\n",
      "epoch: 10, test accuracy: 0.956521749496\n",
      "epoch: 11, test accuracy: 0.956521749496\n",
      "epoch: 12, test accuracy: 0.971014499664\n",
      "epoch: 13, test accuracy: 0.971014499664\n",
      "epoch: 14, test accuracy: 0.971014499664\n",
      "epoch: 15, test accuracy: 0.971014499664\n",
      "epoch: 16, test accuracy: 0.956521749496\n",
      "epoch: 17, test accuracy: 0.956521749496\n",
      "epoch: 18, test accuracy: 0.971014499664\n",
      "epoch: 19, test accuracy: 0.971014499664\n",
      "epoch: 20, test accuracy: 0.985507249832\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, 128, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, 32, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden1, 3, scope=\"logits\",\n",
    "                            activation_fn=None)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.equal(\n",
    "        tf.arg_max(logits, 1), tf.arg_max(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in xrange(epoches):\n",
    "        i = 0\n",
    "        next_batch = next_batch_rdd_maker(train_data, batch_size)\n",
    "        #next_batch = next_sample_rdd_maker(train_data, batch_size)\n",
    "        for X_batch, Y_batch in next_batch():\n",
    "            the_loss, _ = sess.run([loss, training_op], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            i += 1\n",
    "            if i >= n_batch:\n",
    "                break\n",
    "        acc_test = accuracy.eval(feed_dict={X: test_data_dict['features'], Y: test_data_dict['labels']})\n",
    "        print('epoch: {}, test accuracy: {}'.format(epoch+1, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, test accuracy: 0.985507249832\n",
      "epoch: 2, test accuracy: 0.985507249832\n",
      "epoch: 3, test accuracy: 0.985507249832\n",
      "epoch: 4, test accuracy: 0.985507249832\n",
      "epoch: 5, test accuracy: 0.985507249832\n",
      "epoch: 6, test accuracy: 0.985507249832\n",
      "epoch: 7, test accuracy: 0.985507249832\n",
      "epoch: 8, test accuracy: 0.985507249832\n",
      "epoch: 9, test accuracy: 0.985507249832\n",
      "epoch: 10, test accuracy: 0.985507249832\n",
      "epoch: 11, test accuracy: 0.985507249832\n",
      "epoch: 12, test accuracy: 0.985507249832\n",
      "epoch: 13, test accuracy: 0.985507249832\n",
      "epoch: 14, test accuracy: 0.985507249832\n",
      "epoch: 15, test accuracy: 0.985507249832\n",
      "epoch: 16, test accuracy: 0.985507249832\n",
      "epoch: 17, test accuracy: 0.985507249832\n",
      "epoch: 18, test accuracy: 0.985507249832\n",
      "epoch: 19, test accuracy: 0.985507249832\n",
      "epoch: 20, test accuracy: 0.985507249832\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = Adam(lr=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(4,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "for epoch in xrange(epoches):\n",
    "    i = 0\n",
    "    next_batch = next_batch_rdd_maker(train_data, batch_size)\n",
    "    for X_batch, Y_batch in next_batch():\n",
    "        model.train_on_batch(X_batch, Y_batch)\n",
    "        i += 1\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "    evaluate = model.test_on_batch(test_data_dict['features'], test_data_dict['labels'])\n",
    "    print('epoch: {}, test accuracy: {}'.format(epoch+1, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
