{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "fields = [StructField(field_name, DoubleType(), True) \n",
    "          for field_name in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] + \\\n",
    "    [StructField('species', StringType(), True)]\n",
    "    \n",
    "schema = StructType(fields)\n",
    "\n",
    "iris_df = sqlContext.read.csv(\"../dataset/iris.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# features\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# labels\n",
    "str_indexer = StringIndexer(inputCol=\"species\", outputCol=\"species_index\")\n",
    "label_encoder = OneHotEncoder(inputCol=\"species_index\", outputCol=\"labels\")\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(stages=[feature_assembler, str_indexer, label_encoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_pipeline = pipeline.fit(iris_df)\n",
    "iris_dataset_df = iris_pipeline.transform(iris_df, {label_encoder.dropLast: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|         features|       labels|\n",
      "+-----------------+-------------+\n",
      "|[5.1,3.5,1.4,0.2]|(3,[2],[1.0])|\n",
      "|[4.9,3.0,1.4,0.2]|(3,[2],[1.0])|\n",
      "|[4.7,3.2,1.3,0.2]|(3,[2],[1.0])|\n",
      "|[4.6,3.1,1.5,0.2]|(3,[2],[1.0])|\n",
      "|[5.0,3.6,1.4,0.2]|(3,[2],[1.0])|\n",
      "|[5.4,3.9,1.7,0.4]|(3,[2],[1.0])|\n",
      "|[4.6,3.4,1.4,0.3]|(3,[2],[1.0])|\n",
      "|[5.0,3.4,1.5,0.2]|(3,[2],[1.0])|\n",
      "|[4.4,2.9,1.4,0.2]|(3,[2],[1.0])|\n",
      "|[4.9,3.1,1.5,0.1]|(3,[2],[1.0])|\n",
      "|[5.4,3.7,1.5,0.2]|(3,[2],[1.0])|\n",
      "|[4.8,3.4,1.6,0.2]|(3,[2],[1.0])|\n",
      "|[4.8,3.0,1.4,0.1]|(3,[2],[1.0])|\n",
      "|[4.3,3.0,1.1,0.1]|(3,[2],[1.0])|\n",
      "|[5.8,4.0,1.2,0.2]|(3,[2],[1.0])|\n",
      "|[5.7,4.4,1.5,0.4]|(3,[2],[1.0])|\n",
      "|[5.4,3.9,1.3,0.4]|(3,[2],[1.0])|\n",
      "|[5.1,3.5,1.4,0.3]|(3,[2],[1.0])|\n",
      "|[5.7,3.8,1.7,0.3]|(3,[2],[1.0])|\n",
      "|[5.1,3.8,1.5,0.3]|(3,[2],[1.0])|\n",
      "+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_dataset_df.select('features', 'labels').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data = iris_dataset_df.select('features', 'labels').rdd \\\n",
    "    .map(lambda r: {f: r[f].toArray().reshape(1, -1) for f in ['features', 'labels']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = iris_data.sortBy(lambda x: np.random.random()).randomSplit([0.65, 0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[18] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def concate_data(x, y):\n",
    "    dataset = {}\n",
    "    for f in ['features', 'labels']:\n",
    "        dataset[f] = np.concatenate((x[f], y[f]), axis=0)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def next_batch_rdd_maker(data_rdd, batch_size, epoch_limit=None):\n",
    "    data_cnt = data_rdd.count()\n",
    "    per_batch_ratios = [batch_size / data_cnt] * int(math.ceil(data_cnt / batch_size))\n",
    "    def next_batch():\n",
    "        current_epoch = 0\n",
    "        while True:\n",
    "            current_epoch += 1\n",
    "            if epoch_limit is not None and current_epoch > epoch_limit: \n",
    "                break \n",
    "            batches = data_rdd \\\n",
    "                .sortBy(lambda x: np.random.random()) \\\n",
    "                .randomSplit(per_batch_ratios)\n",
    "            for batch in batches:\n",
    "                dataset = batch.reduce(concate_data)\n",
    "                yield dataset['features'], dataset['labels']\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 4.9,  3.1,  1.5,  0.1]]), array([[ 0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_dict = test_data.reduce(concate_data)\n",
    "test_data_dict['features'][:5], test_data_dict['labels'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_batch = next_batch_rdd_maker(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.7,  3. ,  5.2,  2.3]]), array([[ 0.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(next_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 12 7\n"
     ]
    }
   ],
   "source": [
    "epoches = 20\n",
    "data_cnt = train_data.count()\n",
    "batch_size = 12\n",
    "n_batch = data_cnt // batch_size\n",
    "\n",
    "print(data_cnt, batch_size, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, test accuracy: 0.681818187237\n",
      "epoch: 2, test accuracy: 0.931818187237\n",
      "epoch: 3, test accuracy: 0.659090936184\n",
      "epoch: 4, test accuracy: 0.840909063816\n",
      "epoch: 5, test accuracy: 0.977272748947\n",
      "epoch: 6, test accuracy: 0.931818187237\n",
      "epoch: 7, test accuracy: 0.95454543829\n",
      "epoch: 8, test accuracy: 0.95454543829\n",
      "epoch: 9, test accuracy: 0.931818187237\n",
      "epoch: 10, test accuracy: 0.95454543829\n",
      "epoch: 11, test accuracy: 0.95454543829\n",
      "epoch: 12, test accuracy: 0.977272748947\n",
      "epoch: 13, test accuracy: 0.977272748947\n",
      "epoch: 14, test accuracy: 0.977272748947\n",
      "epoch: 15, test accuracy: 0.977272748947\n",
      "epoch: 16, test accuracy: 0.977272748947\n",
      "epoch: 17, test accuracy: 0.931818187237\n",
      "epoch: 18, test accuracy: 0.727272748947\n",
      "epoch: 19, test accuracy: 0.95454543829\n",
      "epoch: 20, test accuracy: 0.931818187237\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, 128, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, 32, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden1, 3, scope=\"logits\",\n",
    "                            activation_fn=None)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.equal(\n",
    "        tf.arg_max(logits, 1), tf.arg_max(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in xrange(epoches):\n",
    "        i = 0\n",
    "        next_batch = next_batch_rdd_maker(train_data, batch_size)\n",
    "        for X_batch, Y_batch in next_batch():\n",
    "            the_loss, _ = sess.run([loss, training_op], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            i += 1\n",
    "            if i >= n_batch:\n",
    "                break\n",
    "        acc_test = accuracy.eval(feed_dict={X: test_data_dict['features'], Y: test_data_dict['labels']})\n",
    "        print('epoch: {}, test accuracy: {}'.format(epoch+1, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, test accuracy: 0.306451618671\n",
      "epoch: 2, test accuracy: 0.741935491562\n",
      "epoch: 3, test accuracy: 0.741935491562\n",
      "epoch: 4, test accuracy: 0.629032254219\n",
      "epoch: 5, test accuracy: 0.854838728905\n",
      "epoch: 6, test accuracy: 0.854838728905\n",
      "epoch: 7, test accuracy: 0.741935491562\n",
      "epoch: 8, test accuracy: 0.677419364452\n",
      "epoch: 9, test accuracy: 0.93548387289\n",
      "epoch: 10, test accuracy: 0.693548381329\n",
      "epoch: 11, test accuracy: 0.983870983124\n",
      "epoch: 12, test accuracy: 0.822580635548\n",
      "epoch: 13, test accuracy: 0.951612889767\n",
      "epoch: 14, test accuracy: 0.967741906643\n",
      "epoch: 15, test accuracy: 1.0\n",
      "epoch: 16, test accuracy: 0.983870983124\n",
      "epoch: 17, test accuracy: 0.93548387289\n",
      "epoch: 18, test accuracy: 1.0\n",
      "epoch: 19, test accuracy: 0.951612889767\n",
      "epoch: 20, test accuracy: 0.951612889767\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = Adam(lr=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(4,)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "for epoch in xrange(epoches):\n",
    "    i = 0\n",
    "    next_batch = next_batch_rdd_maker(train_data, batch_size)\n",
    "    for X_batch, Y_batch in next_batch():\n",
    "        model.train_on_batch(X_batch, Y_batch)\n",
    "        i += 1\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "    metrics = model.test_on_batch(test_data_dict['features'], test_data_dict['labels'])\n",
    "    print('epoch: {}, test accuracy: {}'.format(epoch+1, metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
